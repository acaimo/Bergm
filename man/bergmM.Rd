% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/bergmM.R
\name{bergmM}
\alias{bergmM}
\title{Parameter estimation for Bayesian ERGMs under missing data}
\usage{
bergmM(
  formula,
  burn.in = 100,
  main.iters = 1000,
  aux.iters = 1000,
  prior.mean = NULL,
  prior.sigma = NULL,
  nchains = NULL,
  gamma = 0.5,
  V.proposal = 0.0025,
  seed = NULL,
  startVals = NULL,
  offset.coef = NULL,
  constraints = NULL,
  thin = 1,
  saveEveryX = NULL,
  cut.reject = TRUE,
  saveEveryXName = "partialBergmEstimate.rda",
  imputeAllItr = FALSE,
  imputeLast = TRUE,
  nImp = NULL,
  missingUpdate = NULL,
  imputeData = NULL,
  attributeNames = NULL,
  miceIt = 5,
  onlyKeepImputation = FALSE,
  ...
)
}
\arguments{
\item{formula}{formula; an \code{\link[ergm]{ergm}} formula object,
of the form  <network> ~ <model terms>
where <network> is a \code{\link[network]{network}} object
and <model terms> are \code{ergm-terms}.}

\item{burn.in}{count; number of burn-in iterations for every chain of the population.}

\item{main.iters}{count; number of iterations for every chain of the population.}

\item{aux.iters}{count; number of auxiliary iterations used for network simulation.}

\item{prior.mean}{vector; mean vector of the multivariate Normal prior.
By default set to a vector of 0's. Note that several ergm.terms add more than one parameter to the model. You need to adjust your priors accordingly}

\item{prior.sigma}{square matrix; variance/covariance matrix for the multivariate Normal prior.
By default set to a diagonal matrix with every diagonal entry equal to 100. Note that several ergm.terms add more than one parameter to the model. You need to adjust your priors accordingly}

\item{nchains}{count; number of chains of the population MCMC.
By default set to twice the model dimension (number of model terms).}

\item{gamma}{scalar; parallel adaptive direction sampling move factor.}

\item{V.proposal}{count; diagonal entry for the multivariate Normal proposal.
By default set to 0.0025.}

\item{seed}{count;
random number seed for the Bergm estimation.}

\item{startVals}{vector;
optional starting values for the parameter estimation.}

\item{offset.coef}{vector;
a vector of coefficients for the offset terms.}

\item{constraints}{formula;
A formula specifying one or more constraints on the support of the distribution of the networks being modeled, using syntax similar to the formula argument, on the right-hand side. Multiple constraints may be given, separated by “+” and “-” operators. (See \code{\link[ergm]{ergm}} constraints for the explanation of their semantics.) Together with the model terms in the formula and the reference measure, the constraints define the distribution of networks being modeled.}

\item{thin}{count;
The thinning interval between consecutive observations.}

\item{saveEveryX}{count; If not NULL, the posterior and obtained imputation (only if nImp > 0) will be saved to your working directory at every X iterations.
Note that this slows down estimation and thus X should not be set too low. By default, the saved data will be in 'partialBergmEstimate.rda' and will be overwritten every X iterations and by other calls of bergm() or bergmM().}

\item{cut.reject}{logical;
default TRUE. By default bergm()/bergmM() will save the last accepted theta to the posterior if the new proposal is rejected.
This artificially will increase the auto-correlation of consecutive parameters.}

\item{saveEveryXName}{character; the Name of the partial estimation object.
If you run multiple bergm()/bergmM() calls in the same working directory you should change this name so that the calls do not override each other.}

\item{imputeAllItr}{logical;
default FALSE. If TRUE, missing network and attribute data are imputed after every iteration.
This leads to much (!!!) longer estimation times, but potentially overall better estimation.
It is recommended to initially estimate with imputeAllItr = FALSE to get reasonable starting values and reduce overall estimation time.}

\item{imputeLast}{logical;
default TRUE and in line with Koskinen et al. 2010.
If FALSE, network imputations will be performed with a random draw from the so far accepted parameter values.}

\item{nImp}{count;
number of imputed networks to be returned. If null, no imputed network will be returned.}

\item{missingUpdate}{count;
number of tie updates in each imputation step.
By default equal to the number of missing ties.
Smaller numbers increase speed. Larger numbers lead to better sampling.}

\item{imputeData}{data.frame;
a data.frame with all attribute variables that should be imputed and additional attributes that should be used for the imputation.
All non-numeric variables need to be specified as.factors.
Names of vertex.attributes and variable names need to be identical.}

\item{attributeNames}{character vector,
a vector with the names of all variables that need to be imputed.
These names need to be identical with the vertex.attributes and must be part of the names of the \code{imputeData} data.frame.}

\item{miceIt}{count,
number of iterations in the MICE imputation. Default is 5.}

\item{onlyKeepImputation}{logical,
Should only imputations be returned, and no bergm estimate (only recommended after you made sure that the model estimates properly).}

\item{...}{additional arguments, to be passed to lower-level functions.}
}
\description{
Function to fit Bayesian exponential random graphs models under missing data
using the approximate exchange algorithm.
}
\examples{
\dontrun{
# Load the florentine marriage network
data(florentine)

# Create missing data
set.seed(14021994)
n <- dim(flomarriage[, ])[1]
missNode <- sample(1:n, 1)
flomarriage[missNode, ] <- NA
flomarriage[, missNode] <- NA

# Posterior parameter estimation:
m.flo <- bergmM(flomarriage ~ edges + kstar(2),
                burn.in    = 50,
                aux.iters  = 500,
                main.iters = 1000,
                gamma      = 1.2,
                nImp       = 5)

# Posterior summaries:
summary(m.flo)
}
}
\references{
Caimo, A. and Friel, N. (2011), "Bayesian Inference for Exponential Random Graph Models,"
Social Networks, 33(1), 41-55. \url{https://arxiv.org/abs/1007.5192}

Caimo, A. and Friel, N. (2014), "Bergm: Bayesian Exponential Random Graphs in R,"
Journal of Statistical Software, 61(2), 1-25. \url{https://www.jstatsoft.org/v61/i02}

Koskinen, J.H., Robins, G.L., and Pattison, P.E. (2010), "Analysing exponential
random graph (p-star) models with missing data using Bayesian data augmentation,"
Statistical Methodology 7(3), 366-384.

Krause, R.W., Huisman, M., Steglich, C., and Snijders, T.A. (2020), "Missing data in
cross-sectional networks-An extensive comparison of missing data treatment methods",
Social Networks 62: 99-112.
}
